{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955fd58d",
   "metadata": {},
   "source": [
    "### WEAT(Word Embedding Association Test)\n",
    "\n",
    "- Target : 비교하려는 두 개의 주요 개념 그룹 (ex : 남성관련 - 여성관련)\n",
    "- Attribute : 각 Target 그룹이 얼마나 연관되어 있는지를 측정할 속성 단어 그룹들 (ex: 수학관련속성 - 예술관련 속성 )\n",
    "\n",
    "1) 각 target 단어와 attribute 단어 간 코사인 유사도(cosine similarity) 를 계산\n",
    "\n",
    "2) 그룹 간 평균 유사도 차이를 구함\n",
    "\n",
    "3) 전체 결과에 대해 통계적 효과 크기(effect size) 와 p-value 계산\n",
    "\n",
    "→ 점수가 클수록 편향이 강함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03eafa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb37d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.1  1.2 -2.4  0.5  4.1]\n",
      " [ 3.1  0.5  3.6  1.7  5.8]\n",
      " [ 2.9 -1.3  0.4  1.1  3.7]\n",
      " [ 5.4  2.5  4.6 -1.   3.6]]\n",
      "[[-1.5  0.2 -0.6 -4.6 -5.3]\n",
      " [ 0.4  0.7 -1.9 -4.5 -2.9]\n",
      " [ 0.9  1.4 -2.3 -3.9 -4.7]\n",
      " [ 0.7  0.9 -0.4 -4.1 -3.9]]\n",
      "[[ 2.8  4.2  4.3  0.3  5. ]\n",
      " [ 3.8  3.  -1.2  4.4  4.9]\n",
      " [ 3.7 -0.3  1.2 -2.5  3.9]]\n",
      "[[-0.2 -2.8 -4.7 -4.3 -4.7]\n",
      " [-4.5 -2.1 -3.8 -3.6 -3.1]\n",
      " [-3.6 -3.3 -3.5 -3.7 -4.4]]\n"
     ]
    }
   ],
   "source": [
    "target_X = {\n",
    "    '장미': [4.1, 1.2, -2.4, 0.5, 4.1],\n",
    "    '튤립': [3.1, 0.5, 3.6, 1.7, 5.8],\n",
    "    '백합': [2.9, -1.3, 0.4, 1.1, 3.7],\n",
    "    '데이지': [5.4, 2.5, 4.6, -1.0, 3.6]\n",
    "}\n",
    "target_Y = {\n",
    "    '거미': [-1.5, 0.2, -0.6, -4.6, -5.3],\n",
    "    '모기': [0.4, 0.7, -1.9, -4.5, -2.9],\n",
    "    '파리': [0.9, 1.4, -2.3, -3.9, -4.7],\n",
    "    '메뚜기': [0.7, 0.9, -0.4, -4.1, -3.9]\n",
    "}\n",
    "attribute_A = {\n",
    "    '사랑':[2.8,  4.2, 4.3,  0.3, 5.0],\n",
    "    '행복':[3.8,  3. , -1.2,  4.4, 4.9],\n",
    "    '웃음':[3.7, -0.3,  1.2, -2.5, 3.9]\n",
    "}\n",
    "attribute_B = {\n",
    "    '재난': [-0.2, -2.8, -4.7, -4.3, -4.7],\n",
    "    '고통': [-4.5, -2.1,  -3.8, -3.6, -3.1],\n",
    "    '증오': [-3.6, -3.3, -3.5,  -3.7, -4.4]\n",
    "}\n",
    "\n",
    "X = np.array([v for v in target_X.values()])\n",
    "Y = np.array([v for v in target_Y.values()])\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "A = np.array([v for v in attribute_A.values()])\n",
    "B = np.array([v for v in attribute_B.values()])\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c08f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "# s(w,A,B)는 개별 단어 w가 개념축 A-B에 대해 가지는 편향성을 계산한 값\n",
    "# -2 ~ 2의 값\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "# s(X,A,B)평균 - s(Y,A,B)평균을 전체 표준편차로 나눈 값\n",
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d0b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.932\n",
      "-1.932\n",
      "-1.932\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(round(weat_score(X, Y, A, B), 3))\n",
    "print(round(weat_score(Y, X, A, B), 3))\n",
    "\n",
    "print(round(weat_score(X, Y, B, A), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8add27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장미: 0.6457646122337399\n",
      "거미: -0.794002342033094\n"
     ]
    }
   ],
   "source": [
    "print('장미:', s(target_X['장미'], A, B))\n",
    "print('거미:', s(target_Y['거미'], A, B))\n",
    "\n",
    "#target_X와 attribute_A, attribute_B 사의의 평균값\n",
    "print(s(X, A, B))\n",
    "print(round(np.mean(s(X, A, B)), 3))\n",
    "\n",
    "#target_Y와 attribute_A, attribute_B 사의의 평균값\n",
    "print(s(Y, A, B))\n",
    "print(round(np.mean(s(Y, A, B)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5150acdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7dfa4b174ee0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPm0lEQVR4nO3db2hc153G8eeRLdvVJibEEd5YjjSGTQtxyCYgQkteGNJ01+mGmpYWGmYLpQW92UAKhdIg2KUshoVA2RctlGETurBDS6ENW9KYxGFDQqFJo3TdYMVJCA1SbAdbtSmqEZas6rcv7oz1ZyVbo7kz9x7N9wNifI6He3/Y1qPrc3/3jCNCAIB09RVdAACgPQQ5ACSOIAeAxBHkAJA4ghwAEreziJPecccdUalUijg1ACTrrbfe+mNEDK6dLyTIK5WKJiYmijg1ACTL9tR68yytAEDiCHIASBxBDgCJI8gBIHEEOQAkjiDHltXrUqUi9fVlr/V60RUBvamQ9kOkr16XxsakublsPDWVjSWpWi2uLqAXcUWOLRkfXw7xprm5bB5AdxHk2JLp6dbmAXQOQY4tGR5ubR5A5xDk2JLjx6WBgdVzAwPZPIDuIsixJdWqVKtJIyOSnb3WatzoBIpA1wq2rFoluIEy4IocABJHkANA4ghyAEgcQQ4AiSPIASBxBDkAJI4gB4DEtR3ktvfY/q3t39uetP29PAoDAGxOHg8EzUt6OCKu2O6X9GvbJyLi9RyODQC4ibaDPCJC0pXGsL/xFe0eFwCwObmskdveYfuUpIuSTkbEG+u8Z8z2hO2JmZmZPE4LAFBOQR4Rf4mI+yUdlPSg7XvXeU8tIkYjYnRwcDCP0wIAlHPXSkT8SdIrko7meVwAwMby6FoZtH1b49efkPQ5Se+2e1wAwObk0bVyp6T/tL1D2Q+Gn0XE8zkcFwCwCXl0rbwt6YEcagEAbAFPdgJA4ghyAEgcQQ4AiSPIASBxBDkAJI4gB4DEEeQAkDiCHAASR5ADQOIIcgBIHEEOAIkjyAEgcQQ5ACSOIAeAxBHkAJA4ghwAEkeQA0DiCHIASBxBDgCJI8gBIHFtf/gygOJMXr6qV8/Pafbakvb29+nIgQEdvn1P0WWhywhyIFGTl6/qxPQVLUY2nr22pBPTVySJMO8xLK0AiXr1/Nz1EG9ajGwevYUgBxI1e22ppXlsXwQ5kKi9/et/+240j+2r7b9x23fZfsX2O7YnbT+ZR2EAbuzIgQHt9Oq5nc7m0VvyuNm5KOnbEfE727dKesv2yYh4J4djA9hA84YmXStoO8gj4mNJHzd+/WfbZyQNSSLIgQ47fPseghv5rpHbrkh6QNIbeR4XALCx3ILc9i2Sfi7pWxExu87vj9mesD0xMzOT12kBoOflEuS2+5WFeD0ifrHeeyKiFhGjETE6ODiYx2kBAMqna8WSnpF0JiK+335JAIBW5HFF/pCkr0l62PapxtfnczguAGAT8uha+bUk3/SNAICO4BEwAEgcQQ4AiSPIASBxBDkAJI4gB4DEEeQAkDiCHAASR5ADQOIIcgBIHEEOAIkjyAEgcQQ5ACSOIAeAxOXx4cvogsnLV/mQXQDrIsgTMHn5qk5MX9FiZOPZa0s6MX1FkghzACytpODV83PXQ7xpMbJ5ACDIEzB7bamleQC9hSBPwN7+9f+aNpoH0FtYI0/AkQMDq9bIJWmns/nCXLgkfXhOml+Qdu+SDg1J+/cVVw/QwwjyBDRvaJama+XCJen9KWmpsbQzv5CNJcIcKABBnojDt+8pT4fKh+eWQ7xpaSmbJ8iBrmORFa2bX2htHkBHEeRo3e5drc0D6CiCHK07NCT1rfmn09eXzQPoOtbI0brmOjhdK0ApEOTYmv37CG6gJHJZWrH9rO2Ltk/ncTwAwObltUb+Y0lHczoWAKAFuQR5RLwm6XIexwIAtKZrXSu2x2xP2J6YmZnp1mnRbfW6VKlkXSyVSjYG0FFdC/KIqEXEaESMDg4Oduu06KZ6XRobk6ampIjsdWyMMAc6jD5y5Gd8XJpbs0f63Fw2D6BjCHLkZ3q6tXkAucir/fAnkn4j6VO2z9r+Zh7HRWKGh1ubB5CLvLpWHo+IOyOiPyIORsQzeRwXiTl+XBpYs0f6wEA2D6BjWFpBfqpVqVaTRkYkO3ut1bJ5AB3DI/rIV7VKcANdxhU5ACSOIAeAxBHkAJA4ghwAEkeQA0DiCHIASBxBDgCJI8gBIHEEOQAkjiAHgMQR5ACQOIIcABJHkANA4ghyAEgcQQ4AiSPIASBxBDkAJI4gB4DEEeQAkDiCHAASx4cvA1h24ZL04TlpfkHavUs6NCTt31d0VbgJghxA5sIl6f0paWkpG88vZGOJMC85llYAZD48txziTUtL2TxKLZcgt33U9nu2P7D93TyOCaDL5hdam0dptB3ktndI+qGkRyXdI+lx2/e0e1wAXbZ7V2vzKI08rsgflPRBRPwhIhYk/VTSsRyOC6CbDg1JfWsioa8vm0ep5RHkQ5I+WjE+25gDkJL9+6RPjixfge/elY250Vl6XetasT0maUyShoeHu3VaAK3Yv4/gTlAeV+TnJN21YnywMbdKRNQiYjQiRgcHB3M4LQBAyifI35R0t+1DtndJ+qqkX+ZwXADAJrS9tBIRi7afkPSipB2Sno2IybYrAwBsSi5r5BHxgqQX8jgWAKA1PNkJoPPqdalSydoZK5VsjNyw1wqAzqrXpbExaW4uG09NZWNJqlaLq2sb4YocQGeNjy+HeNPcXDaPXBDkADprerq1ebSMIAfQWRs9AMiDgbkhyAF01vHj0sDA6rmBgWweuSDIAXRWtSrVatLIiGRnr7UaNzpzRNcKgM6rVgnuDuKKHAASR5ADQOIIcgBIHGvkKL8Ll7IPAJ5fyD7s4NAQe2YDKxDkKLcLl6T3p5Y/3X1+IRtLhDnQwNIKyu3Dc8sh3rS0lM0DkESQo+zmF1qbB3oQQV5GbPm5rPlBwJudB3oQQV42zS0/p6akiOUtP3s1zA8NZT/QVurry+YBSCLIy4ctP1fbv0/65MjyFfjuXdmYG53AdXStlE1Zt/wssgVw/z6CG7gBrsjLpoxbfjZbAJs3GJstgBcuFVcTgOsI8rIp45aftAACpUaQl00Zt/ykBRAoNdbIy6hsW37u3rV+aNMCCJQCV+S4OVoAgVLjihw31+wYYeMqoJQIcmwOLYBAabW1tGL7K7YnbS/ZHs2rKADA5rW7Rn5a0pckvZZDLQCALWhraSUizkiS7XyqAQC0rGtdK7bHbE/YnpiZmenWaQFg27tpkNt+2fbpdb6OtXKiiKhFxGhEjA4ODm69YgBIUCd3p77p0kpEPJLf6QCg9zR3p25ubNrcnVrK59k/HggCgA7r9O7U7bYfftH2WUmfkfQr2y/mUxYAbB+d3p26rSCPiOci4mBE7I6I/RHx9/mUBQDbR6d3p2ZpBQA6rNO7UxPkANBhnd6dmr1WAKALOrk7NVfkAJA4ghwAEkeQA0DiCHIASBxBDgCJI8gBIHEEOQAkjiAHgMQR5ACQOIIcABJHkANA4pLZa2Xy8lW9en5Os9eWtLe/T0cODOjw7XuKLgsACpdEkE9evqoT01e0GNl49tqSTkxfkSTCHEDPS2Jp5dXzc9dDvGkxsnkA6HVJBPnstaWW5gGglyQR5Hv71y9zo3kA6CVJJOGRAwPa6dVzO53NA0CvS+JmZ/OGJl0rAPK0XbrhkghyKQvzFP+AAZTTduqGS2JpBQDytp264QhyAD1pO3XDEeQAetJ26oZrq2LbT9t+1/bbtp+zfVtOdQFAR22nbrh2f/SclHRvRNwn6X1JT7VfEgB03uHb9+jR4VuuX4Hv7e/To8O3JHejU2qzayUiXloxfF3Sl9srBwC6Z7t0w+W5GPQNSSdyPB4AYBNuekVu+2VJf73Ob41HxH833jMuaVFS/QbHGZM0JknDw8NbKhYA8P/dNMgj4pEb/b7tr0t6TNJnIyI2el9E1CTVJGl0dHTD9wEAWtPWGrnto5K+I+lIRKTXRQ8A20C7a+Q/kHSrpJO2T9n+UQ41AQBa0G7Xyt/kVQgAYGvSe4QJALAKQQ4AiUsqyOt1qVKR+vqy1/qGzY4A0DuS2Y+8XpfGxqS5Rm/M1FQ2lqRqtbi6AKBoyVyRj48vh3jT3Fw2DwC9LJkgn55ubR4AekUyQb7RU/087Q+g1yUT5MePSwNrtgkeGMjmAaCXJRPk1apUq0kjI5KdvdZq3OgEgGS6VqQstAluAFgtmStyAMD6CHIASBxBDgCJI8gBIHEEOQAkzjf4dLbOndSekTTV9RNv7A5Jfyy6iHVQV2uoqzXU1Zoy1DUSEYNrJwsJ8rKxPRERo0XXsRZ1tYa6WkNdrSlrXRJLKwCQPIIcABJHkGdqRRewAepqDXW1hrpaU9a6WCMHgNRxRQ4AiSPIASBxBHmD7X+1/bbtU7Zfsn2g6JokyfbTtt9t1Pac7duKrkmSbH/F9qTtJduFt2TZPmr7Pdsf2P5u0fVIku1nbV+0fbroWlayfZftV2y/0/g7fLLomiTJ9h7bv7X9+0Zd3yu6ppVs77D9v7afL7qWtQjyZU9HxH0Rcb+k5yX9c8H1NJ2UdG9E3CfpfUlPFVxP02lJX5L0WtGF2N4h6YeSHpV0j6THbd9TbFWSpB9LOlp0EetYlPTtiLhH0qcl/VNJ/rzmJT0cEX8r6X5JR21/utiSVnlS0pmii1gPQd4QEbMrhn8lqRR3gSPipYhYbAxfl3SwyHqaIuJMRLxXdB0ND0r6ICL+EBELkn4q6VjBNSkiXpN0ueg61oqIjyPid41f/1lZOA0VW5UUmSuNYX/jqxTfh7YPSvoHSf9RdC3rIchXsH3c9keSqirPFflK35B0ougiSmhI0kcrxmdVgmBKge2KpAckvVFwKZKuL1+cknRR0smIKEVdkv5d0nckLRVcx7p6Kshtv2z79DpfxyQpIsYj4i5JdUlPlKWuxnvGlf2XuF6mupAu27dI+rmkb635H2lhIuIvjeXNg5IetH1vwSXJ9mOSLkbEW0XXspGkPuqtXRHxyCbfWpf0gqR/6WA5192sLttfl/SYpM9GFxv/W/jzKto5SXetGB9szGEDtvuVhXg9In5RdD1rRcSfbL+i7B5D0TeLH5L0Bdufl7RH0l7b/xUR/1hwXdf11BX5jdi+e8XwmKR3i6plJdtHlf2X7gsRMVd0PSX1pqS7bR+yvUvSVyX9suCaSsu2JT0j6UxEfL/oeppsDza7smx/QtLnVILvw4h4KiIORkRF2b+t/ylTiEsE+Ur/1lg2eFvS3ym7Q10GP5B0q6STjdbIHxVdkCTZ/qLts5I+I+lXtl8sqpbGzeAnJL2o7MbdzyJisqh6mmz/RNJvJH3K9lnb3yy6poaHJH1N0sONf1OnGlebRbtT0iuN78E3la2Rl67Vr4x4RB8AEscVOQAkjiAHgMQR5ACQOIIcABJHkANA4ghyAEgcQQ4Aifs/PRpqvx1brYkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc_A = pca.fit_transform(A)\n",
    "pc_B = pca.fit_transform(B)\n",
    "pc_X = pca.fit_transform(X)\n",
    "pc_Y = pca.fit_transform(Y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(pc_A[:,0],pc_A[:,1], c='blue', label='A')\n",
    "ax.scatter(pc_B[:,0],pc_B[:,1], c='red', label='B')\n",
    "ax.scatter(pc_X[:,0],pc_X[:,1], c='skyblue', label='X')\n",
    "ax.scatter(pc_Y[:,0],pc_Y[:,1], c='pink', label='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee1cee",
   "metadata": {},
   "source": [
    "## 사전학습된 Word Embedding에 WEAT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd22d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "300\n",
      "(500000, 300)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '~/aiffel/weat' \n",
    "model_dir = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 50만개의 단어만 활용합니다. 메모리가 충분하다면 limit 파라미터값을 생략하여 300만개를 모두 활용할 수 있습니다. \n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)\n",
    "\n",
    "# print(len(w2v.vocab))   # Gensim 3.X 버전까지는 w2v.vocab을 직접 접근할 수 있습니다. \n",
    "print(len(w2v.index_to_key))   # Gensim 4.0부터는 index_to_key를 활용해 vocab size를 알 수 있습니다. \n",
    "print(len(w2v['I']))                    # 혹은 단어를 key로 직접 vector를 얻을 수 있습니다. \n",
    "print(w2v.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b131469d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elementary', 0.7868632078170776),\n",
       " ('schools', 0.7411909103393555),\n",
       " ('elementary_schools', 0.6597153544425964),\n",
       " ('kindergarten', 0.6529811024665833),\n",
       " ('eighth_grade', 0.6488089561462402),\n",
       " ('School', 0.6477997303009033),\n",
       " ('teacher', 0.63824063539505),\n",
       " ('students', 0.6301522850990295),\n",
       " ('classroom', 0.6281620264053345),\n",
       " ('Schools', 0.6172096133232117)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5875736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2624874"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과학, 예술 : 남자 , 여자\n",
    "target_X = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "target_Y = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "attribute_A = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "attribute_B = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d031dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6909266"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인스턴스 음식 , 신선항 음식\n",
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['junk', 'canned', 'convenience', 'frozen', 'fast']\n",
    "attribute_B = ['health', 'beneficial', 'good', 'nourishing', 'nutritious']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe754609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05137869"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['book', 'essay', 'dictionary', 'magazine', 'novel']\n",
    "attribute_B = ['news', 'report', 'statement', 'broadcast', 'word']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa91566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.637953"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X = [\"weightlifting\", \"deadlift\", \"squat\",  \"barbell\", \"dumbbell\", \"powerlifting\", \"strength\", \"muscle\", \"resistance\"]\n",
    "target_Y = [\"running\", \"jogging\", \"cycling\", \"aerobics\", \"treadmill\", \"elliptical\", \"cardio\",             \"swimming\", \"rowing\"]\n",
    "attribute_A = [\"healthy\", \"energizing\", \"beneficial\", \"fit\", \"strong\", \"refreshing\", \"motivating\", \"productive\"]\n",
    "attribute_B =[\"tiring\", \"boring\", \"painful\", \"draining\", \"injury\", \"sore\", \"exhausting\", \"unpleasant\"]\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d3b453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3719296"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X = [\"weightlifting\", \"deadlift\", \"squat\",  \"barbell\", \"dumbbell\", \"powerlifting\", \"strength\", \"muscle\", \"resistance\"]\n",
    "target_Y = [\"running\", \"jogging\", \"cycling\", \"aerobics\", \"treadmill\", \"elliptical\", \"cardio\",             \"swimming\", \"rowing\"]\n",
    "attribute_A = [\"healthy\", \"refreshing\", \"energizing\", \"enjoyable\", \"beneficial\", \"motivating\", \"invigorating\", \"productive\"]\n",
    "attribute_B =[\"tiring\", \"painful\", \"boring\", \"exhausting\", \"injury\", \"sore\", \"unpleasant\", \"draining\"]\n",
    "\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbb2903a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95830524"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X = [\"weightlifting\", \"deadlift\", \"squat\",  \"barbell\", \"dumbbell\", \"powerlifting\", \"strength\", \"muscle\", \"resistance\"]\n",
    "target_Y = [\"running\", \"jogging\", \"cycling\", \"aerobics\", \"treadmill\", \"elliptical\", \"cardio\",             \"swimming\", \"rowing\"]\n",
    "attribute_A = [\"strong\", \"power\", \"tough\", \"rugged\", \"aggressive\", \"bold\", \"dominant\", \"assertive\"]\n",
    "attribute_B =[\"graceful\", \"gentle\", \"delicate\", \"soft\", \"nurturing\", \"sensitive\", \"emotional\", \"caring\"]\n",
    "\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b902168a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2251108"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X = [\"weightlifting\", \"deadlift\", \"squat\",  \"barbell\", \"dumbbell\", \"powerlifting\", \"strength\", \"muscle\", \"resistance\"]\n",
    "target_Y = [\"running\", \"jogging\", \"cycling\", \"aerobics\", \"treadmill\", \"elliptical\", \"cardio\", \"swimming\", \"rowing\"]\n",
    "attribute_A = [\"muscle\", \"bulk\", \"strength\", \"physique\", \"tone\", \"mass\", \"power\", \"build\"]\n",
    "attribute_B =[\"weightloss\", \"burn\", \"calories\", \"sweat\", \"cardio\", \"aerobic\", \"fat\", \"endurance\"]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34218df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 완료\n"
     ]
    }
   ],
   "source": [
    "#메모리를 다시 비워줍시다.\n",
    "del w2v\n",
    "print(\"삭제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c458531",
   "metadata": {},
   "source": [
    "## 직접 만드는 Word Embedding에 WEAT 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5324020",
   "metadata": {},
   "source": [
    "형태소 분석기를 이용하여 품사가 명사인 경우, 해당 단어를 추출하기\n",
    "추출된 결과로 embedding model 만들기\n",
    "TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f72c11",
   "metadata": {},
   "source": [
    "### 1. 형태소 분석기를 이용하여 품사가 명사인 경우, 해당 단어를 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03f03141",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54/3886233908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 어간 추출, 표준화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# [('오늘', 'Noun'), ('은', 'Josa'), ('날씨', 'Noun'), ('가', 'Josa'), ('정말', 'Adverb'), ('좋다', 'Adjective'), ('!', 'Punctuation')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         tokens = self.jki.tokenize(\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 약 15분정도 걸립니다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True) # 어간 추출, 표준화\n",
    "        # [('오늘', 'Noun'), ('은', 'Josa'), ('날씨', 'Noun'), ('가', 'Josa'), ('정말', 'Adverb'), ('좋다', 'Adjective'), ('!', 'Punctuation')]\n",
    "        \n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" 등을 포함할 수도 있습니다.\n",
    "                res.append(w[0])    # 명사일 때만 tokenized 에 저장하게 됩니다. \n",
    "        tokenized.append(res)\n",
    "\n",
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a14a43",
   "metadata": {},
   "source": [
    "### 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])\n",
    "\n",
    "# Gensim 3.X 에서는 아래와 같이 생성합니다. \n",
    "# model = Word2Vec(tokenized, size=100, window=5, min_count=3, sg=0)  \n",
    "# model.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4a412",
   "metadata": {},
   "source": [
    "### 3. TF-IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245416f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# 2개의 파일을 처리하는데 10분 가량 걸립니다. \n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "# 2개의 파일을 처리하는데 10분 가량 걸립니다. \n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)\n",
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])\n",
    "\n",
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "#상위 100개의 단어들 중 중복되는 단어를 제외하고 상위 n(=15)개의 단어를 추출\n",
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9117d7",
   "metadata": {},
   "source": [
    "예술영화라는 개념을 가장 잘 대표하는 단어들을 art에서 잘 나타나지만, 다른데서는 나타나지 않는 단어들을 뽑자\n",
    "\n",
    "-> TF-IDF\n",
    "\n",
    "코퍼스에서 자주 나타나는(TF가 높은) 단어이지만, \n",
    "다른 코퍼스에까지 두루 걸쳐 나오지는 않는(IDF가 높은) 단어를 선정하고 싶은 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)\n",
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])\n",
    "\n",
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7667f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#상위 100개의 단어들 중 중복되는 단어를 제외하고 상위 n(=15)개의 단어를 추출\n",
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86cf22",
   "metadata": {},
   "source": [
    "**장르별 대표 단어를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_txt = ['synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_action.txt', 'synopsis_comedy.txt', 'synopsis_war.txt', 'synopsis_horror.txt']\n",
    "genre_name = ['드라마', '멜로로맨스', '액션', '코미디', '전쟁', '공포(호러)']\n",
    "\n",
    "# 약 10분정도 걸립니다.\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))\n",
    "    \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "m = [X[i].tocoo() for i in range(X.shape[0])] # TF-IDF로 표현한 sparse matrix list를 가져옵니다\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True) #단어들을 TF-IDF가 높은 순으로 정렬합니다.\n",
    "    \n",
    "    \n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()\n",
    "\n",
    "# <-- 장르마다 중복된 단어 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229c0f7",
   "metadata": {},
   "source": [
    "### 4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c57ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traget_X는 art, target_Y는 gen, attribute_A는 '드라마', attribute_B는 '액션' \n",
    "\n",
    "#target_X 는 art, target_Y 는 gen으로 고정하고 attribute_A, attribute_B를 바꿔가면서 구해봅시다. \n",
    "# 구한 결과를 21x21 매트릭스 형태로 표현해서 matrix 라는 변수에 담아봅시다.\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "\n",
    "\n",
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
