{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082a6f25",
   "metadata": {},
   "source": [
    "- 케라스 토크나이저를 통해 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f10a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'likes': 1, 'to': 2, 'watch': 3, 'movies': 4, 'mary': 5, 'john': 6, 'too': 7, 'also': 8, 'football': 9, 'games': 10}\n",
      "Bag of Words : {'john': 1, 'likes': 3, 'to': 2, 'watch': 2, 'movies': 2, 'mary': 2, 'too': 1, 'also': 1, 'football': 1, 'games': 1}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence) # 단어장 생성\n",
    "print(tokenizer.word_index) # 각 단어에 대한 인코딩 결과 출력\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 0번 index는  padding용\n",
    "\n",
    "bow = dict(tokenizer.word_counts) # 각 단어와 각 단어의 빈도를 bow에 저장\n",
    "\n",
    "print(\"Bag of Words :\", bow) # bow 출력\n",
    "print('단어장(Vocabulary)의 크기 :', len(tokenizer.word_counts)) # 중복을 제거한 단어들의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fafeb",
   "metadata": {},
   "source": [
    "- scikit-learn CountVectorizer 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca7d2f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1 3 2 2 2 1 2]]\n",
      "각 단어의 인덱스 : {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(sentence).toarray()\n",
    "\n",
    "print('Bag of Words : ', bow) # 코퍼스로부터 각 단어의 빈도수를 기록한다.\n",
    "print('각 단어의 인덱스 :', vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "\n",
    "print('단어장(Vocabulary)의 크기 :', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3040c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[0 0 0 1 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 1 0 1 0 1]]\n",
      "각 단어의 인덱스 : {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'John likes to watch movies',\n",
    "    'Mary likes movies too',\n",
    "    'Mary also likes to watch football games',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print('Bag of Words : ', vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록.\n",
    "print('각 단어의 인덱스 :', vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "print('단어장(Vocabulary)의 크기 :', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519eddfe",
   "metadata": {},
   "source": [
    "**원-핫 벡터**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index)) # 0으로 초기화\n",
    "       index = word2index[word]\n",
    "       one_hot_vector[index-1] = 1\n",
    "       return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d538a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n",
      "[[1, 2, 1, 4]]\n",
      "[[[0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 케라스를 통한 원-핫 인코딩(one-hot encoding)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력.\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
    "encoded = t.texts_to_sequences([sub_text])\n",
    "print(encoded)\n",
    "\n",
    "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde089e",
   "metadata": {},
   "source": [
    "**DTM(Document-Term Matrix)**\n",
    "문서의 수가 많아지면 많아질수록, 통합 단어장의 크기도 커지게 되어서 \n",
    "DTM은 결국 문서 벡터와 단어 벡터 모두 대부분의 값이 0이 되는 성질\n",
    "\n",
    "BoW를 기반으로 문서를 비교\n",
    "\n",
    "DTM에서 문서의 수와 단어의 수가 계속 늘어날수록, 행과 열은 대부분의 값이 0을 가진다는 특징이 있습니다. 이는 저장 공간 측면에서 낭비\n",
    "단어의 빈도에만 집중하는 방법 자체의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa9b2f",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "DTM(TF)의 각 단어에 IDF 값 곱하기 -> TF-IDF 행렬\n",
    "<단점>\n",
    "- 토픽 한계\n",
    "- 유사어 처리 안됨\n",
    "--> LSA : 텍스트 데이터에서 숨겨진 주제(Topic)를 추출하는 데 사용되는 차원 축소 및 주제 모델링 기법\n",
    "--> Word Embeddings \n",
    "--> ConceptNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf244a",
   "metadata": {},
   "source": [
    "**문서유사도**\n",
    "- cosine similarity with Bag of words\n",
    "- cosine similarity with TF-IDF Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750d9091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "0.67\n",
      "1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "doc1 = np.array([0,1,1,1]) # 문서1 벡터\n",
    "doc2 = np.array([1,0,1,1]) # 문서2 벡터\n",
    "doc3 = np.array([2,0,2,2]) # 문서3 벡터\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print('{:.2f}'.format(cos_sim(doc1, doc2))) #문서1과 문서2의 코사인 유사도\n",
    "print('{:.2f}'.format(cos_sim(doc1, doc3))) #문서1과 문서3의 코사인 유사도\n",
    "print('{:.2f}'.format(cos_sim(doc2, doc3))) #문서2과 문서3의 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bea3c2",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea8e5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 13\n",
      "['James', 'John', 'Mary', 'TV', 'also', 'and', 'football', 'games', 'likes', 'movies', 'to', 'too', 'watch']\n",
      "TF:    James  John  Mary  TV  also  and  football  games  likes  movies  to  too  \\\n",
      "0      0     1     1   0     0    1         0      0      2       2   2    1   \n",
      "1      1     0     0   1     0    0         0      0      1       0   1    0   \n",
      "2      0     0     1   0     1    0         1      1      1       0   1    0   \n",
      "\n",
      "   watch  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "IDF:                IDF\n",
      "James     1.405465\n",
      "John      1.405465\n",
      "Mary      1.000000\n",
      "TV        1.405465\n",
      "also      1.405465\n",
      "and       1.405465\n",
      "football  1.405465\n",
      "games     1.405465\n",
      "likes     0.712318\n",
      "movies    1.405465\n",
      "to        0.712318\n",
      "too       1.405465\n",
      "watch     0.712318\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>James</th>\n",
       "      <th>John</th>\n",
       "      <th>Mary</th>\n",
       "      <th>TV</th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>likes</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.424636</td>\n",
       "      <td>2.81093</td>\n",
       "      <td>1.424636</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.712318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      James      John  Mary        TV      also       and  football     games  \\\n",
       "0  0.000000  1.405465   1.0  0.000000  0.000000  1.405465  0.000000  0.000000   \n",
       "1  1.405465  0.000000   0.0  1.405465  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000   1.0  0.000000  1.405465  0.000000  1.405465  1.405465   \n",
       "\n",
       "      likes   movies        to       too     watch  \n",
       "0  1.424636  2.81093  1.424636  1.405465  0.712318  \n",
       "1  0.712318  0.00000  0.712318  0.000000  0.712318  \n",
       "2  0.712318  0.00000  0.712318  0.000000  0.712318  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "import pandas as pd\n",
    "\n",
    "docs = [\n",
    "  'John likes to watch movies and Mary likes movies too',\n",
    "  'James likes to watch TV',\n",
    "  'Mary also likes to watch football games',  \n",
    "]\n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "print('단어장의 크기 :', len(vocab))\n",
    "print(vocab)\n",
    "\n",
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    " \n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc    \n",
    "    return log(N/(df + 1)) + 1\n",
    " \n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)\n",
    "\n",
    "# TF\n",
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        \n",
    "        result[-1].append(tf(t, d))\n",
    "        \n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "print('TF:', tf_)\n",
    "\n",
    "#IDF\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns=[\"IDF\"])\n",
    "idf_\n",
    "print('IDF:', idf_)\n",
    "\n",
    "#TF-IDF\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        \n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac53336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>and</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>james</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>tv</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.379832</td>\n",
       "      <td>0.244551</td>\n",
       "      <td>0.643111</td>\n",
       "      <td>0.189916</td>\n",
       "      <td>0.321556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.338381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "      <td>0.353642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       also       and  football     games     james      john     likes  \\\n",
       "0  0.000000  0.321556  0.000000  0.000000  0.000000  0.321556  0.379832   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.572929  0.000000  0.338381   \n",
       "2  0.464997  0.000000  0.464997  0.464997  0.000000  0.000000  0.274634   \n",
       "\n",
       "       mary    movies        to       too        tv     watch  \n",
       "0  0.244551  0.643111  0.189916  0.321556  0.000000  0.189916  \n",
       "1  0.000000  0.000000  0.338381  0.000000  0.572929  0.338381  \n",
       "2  0.353642  0.000000  0.274634  0.000000  0.000000  0.274634  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "  'John likes to watch movies and Mary likes movies too',\n",
    "  'James likes to watch TV',\n",
    "  'Mary also likes to watch football games',  \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "vocab = list(tfidfv.vocabulary_.keys()) # 단어장을 리스트로 저장\n",
    "vocab.sort() # 단어장을 알파벳 순으로 정렬\n",
    "\n",
    "# TF-IDF 행렬에 단어장을 데이터프레임의 열로 지정하여 데이터프레임 생성\n",
    "tfidf_ = pd.DataFrame(tfidfv.transform(corpus).toarray(), columns = vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fe2bd",
   "metadata": {},
   "source": [
    "**LSA(Latent Semantic Analysis)**\n",
    "선형 대수 기반의 주제 모델링 방법\n",
    "\n",
    "**문서-단어 행렬(DTM)**에 **특이값 분해(SVD:Singular Value Decompotion)**를 적용하여 **잠재적인 의미 공간(latent semantic space)**을 찾음\n",
    "\n",
    "단어 간 의미적 유사성을 포착하는 데 유리\n",
    "\n",
    "(Uk, VkT, S)는 각각 '문서들과 관련된 의미들을 표현한 행렬', '단어들과 관련된 의미를 표현한 행렬' , '각 의미의 중요도를 표현한 행렬' \n",
    "VkT 행렬의 k열은 전체 코퍼스로부터 얻어낸 **k개의 주요 주제(topic)** \n",
    "\n",
    "SVD : 행렬의 크기를 감소시킨다. \n",
    "      가치가 높은순으로 정렬 -> 복원(가치가 높은 것만 복원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f281e54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054983, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "\n",
    "csv_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/abcnews-date-text.csv'\n",
    "\n",
    "data = pd.read_csv(csv_filename, on_bad_lines='skip')\n",
    "data.shape\n",
    "\n",
    "text = data[['headline_text']].copy()\n",
    "text.head()\n",
    "\n",
    "text.nunique() # 중복을 제외하고 유일한 시퀀스를 가지는 샘플의 개수를 출력\n",
    "\n",
    "text.drop_duplicates(inplace=True) # 중복 샘플 제거\n",
    "text.reset_index(drop=True, inplace=True)\n",
    "text.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433d25d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, community, broadcasting, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, calls, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0   [aba, decides, community, broadcasting, licence]\n",
       "1    [act, fire, witnesses, must, aware, defamation]\n",
       "2     [g, calls, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 토크나이저를 이용해서 토큰화\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43a73e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [aba, decide, community, broadcast, licence]\n",
      "1    [act, fire, witness, must, aware, defamation]\n",
      "2       [call, infrastructure, protection, summit]\n",
      "3            [air, staff, aust, strike, pay, rise]\n",
      "4    [air, strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 단어 정규화. 3인칭 단수 표현 -> 1인칭 변환, 과거형 동사 -> 현재형 동사 등을 수행한다.\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "# 길이가 1 ~ 2인 단어는 제거.\n",
    "text = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816dc8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba decide community broadcast licence',\n",
       " 'act fire witness must aware defamation',\n",
       " 'call infrastructure protection summit',\n",
       " 'air staff aust strike pay rise',\n",
       " 'air strike affect australian travellers']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 수행)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(text[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "train_data = detokenized_doc\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f8df53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 상위 5000개의 단어만 사용하여  DTM 생성\n",
    "c_vectorizer = CountVectorizer(stop_words='english', max_features = 5000)\n",
    "document_term_matrix = c_vectorizer.fit_transform(train_data)\n",
    "print('행렬의 크기 :',document_term_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0b4e4",
   "metadata": {},
   "source": [
    "**scikit-learn TruncatedSVD 활용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca8b7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "Topic 1: [('police', 0.74635), ('man', 0.4535), ('charge', 0.21091), ('new', 0.14089), ('court', 0.11147)]\n",
      "Topic 2: [('man', 0.69424), ('charge', 0.30028), ('court', 0.16674), ('face', 0.11591), ('murder', 0.10654)]\n",
      "Topic 3: [('new', 0.83673), ('plan', 0.23648), ('say', 0.18275), ('govt', 0.11054), ('council', 0.10968)]\n",
      "Topic 4: [('say', 0.73893), ('plan', 0.35811), ('govt', 0.16605), ('council', 0.12836), ('urge', 0.07702)]\n",
      "Topic 5: [('plan', 0.73386), ('council', 0.17595), ('govt', 0.14235), ('urge', 0.09338), ('water', 0.08441)]\n",
      "Topic 6: [('govt', 0.54915), ('court', 0.26289), ('urge', 0.22067), ('fund', 0.20663), ('face', 0.17575)]\n",
      "Topic 7: [('charge', 0.52349), ('court', 0.44652), ('face', 0.37521), ('plan', 0.12095), ('murder', 0.11957)]\n",
      "Topic 8: [('win', 0.57842), ('court', 0.38812), ('kill', 0.20302), ('crash', 0.15088), ('australia', 0.09559)]\n",
      "Topic 9: [('win', 0.67033), ('charge', 0.42204), ('cup', 0.09574), ('australia', 0.09296), ('world', 0.08933)]\n",
      "Topic 10: [('council', 0.60733), ('charge', 0.31954), ('kill', 0.31079), ('crash', 0.25168), ('water', 0.12844)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = 10\n",
    "lsa_model = TruncatedSVD(n_components = n_topics)\n",
    "lsa_model.fit_transform(document_term_matrix)\n",
    "print(lsa_model.components_.shape)\n",
    "\n",
    "terms = c_vectorizer.get_feature_names_out() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "print('단어 집합의 크기 :',term.shape)\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "        # topic.argsort : 가중치를 오름차순으로 정렬, [::-1]->내림차순으로 출력\n",
    "\n",
    "get_topics(lsa_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef3b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00e55a",
   "metadata": {},
   "source": [
    "**scikit-learn LDA Model 활용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "482da7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬의 크기 : (1054983, 5000)\n",
      "(10, 5000)\n",
      "단어 집합의 크기 : (5000,)\n",
      "Topic 1: [('australia', 9359.06334), ('sydney', 5854.97288), ('attack', 4784.76322), ('change', 4193.63035), ('year', 3924.88997)]\n",
      "Topic 2: [('government', 6344.07413), ('charge', 5947.12292), ('man', 4519.7974), ('state', 3658.16422), ('live', 3625.10473)]\n",
      "Topic 3: [('australian', 7666.65651), ('say', 7561.01807), ('police', 5513.22932), ('home', 4048.38409), ('report', 3796.04446)]\n",
      "Topic 4: [('melbourne', 5298.35047), ('south', 4844.59835), ('death', 4281.78433), ('china', 3214.44581), ('women', 3029.28443)]\n",
      "Topic 5: [('win', 5704.0914), ('canberra', 4322.0963), ('die', 4025.63057), ('open', 3771.65243), ('warn', 3577.47151)]\n",
      "Topic 6: [('court', 5246.3124), ('world', 4536.86331), ('country', 4166.34794), ('woman', 3983.97748), ('crash', 3793.50267)]\n",
      "Topic 7: [('election', 5418.5038), ('adelaide', 4864.95604), ('house', 4478.6135), ('school', 3966.82676), ('2016', 3955.11155)]\n",
      "Topic 8: [('trump', 8189.58575), ('new', 6625.2724), ('north', 3705.40987), ('rural', 3521.42659), ('donald', 3356.26657)]\n",
      "Topic 9: [('perth', 4552.8151), ('kill', 4093.61782), ('break', 2695.71958), ('budget', 2596.93268), ('children', 2586.01957)]\n",
      "Topic 10: [('queensland', 5552.68506), ('coast', 3825.32603), ('tasmanian', 3550.75997), ('shoot', 3185.71575), ('service', 2695.21462)]\n"
     ]
    }
   ],
   "source": [
    "# 상위 5,000개의 단어만 사용\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "# TF-IDF 행렬의 크기를 확인해봅시다.\n",
    "print('행렬의 크기 :', tf_idf_matrix.shape)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_model.fit_transform(tf_idf_matrix)\n",
    "print(lda_model.components_.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names_out() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "print('단어 집합의 크기 :',terms.shape)\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "        # topic.argsort : 가중치를 오름차순으로 정렬, [::-1]->내림차순으로 출력\n",
    "\n",
    "get_topics(lda_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9087c",
   "metadata": {},
   "source": [
    "**soynlp 형태소 분석기 : 비지도학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71504c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp의 응집 확률(cohesion probability)\n",
    "# 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높습니다\n",
    "\n",
    "# soynlp의 브랜칭 엔트로피(branching entropy)\n",
    "# 주어진 문자열에서 다음 문자가 등장할 수 있는 가능성을 판단하는 척도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a68539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 2.166 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "txt_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/2016-10-20.txt'\n",
    "\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "# 말뭉치에 대해서 다수의 문서로 분리\n",
    "corpus = DoublespaceLineCorpus(txt_filename)\n",
    "len(corpus)\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9128554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄워쓰기가 잘 된 문장 : L토크나이저 사용\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e58794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄워쓰기가 잘 안된 문장 : 최대점수 토크나이저 \n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
