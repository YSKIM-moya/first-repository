{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a087d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9051f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47cb89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7760d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)    #?.!, 기준으로 공백을 앞뒤로 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)          # 중복된 공백 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)  # 알파벳,?.!, 이외는 공백으로 치환\n",
    "\n",
    "    sentence = sentence.strip()                          # 불필요한 공백 제거\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684e2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9473980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: go away !\n",
      "Spanish: <start> salga de aqu ! <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for pair in raw[:num_examples]:\n",
    "    eng, spa = pair.split(\"\\t\")\n",
    "\n",
    "    enc_corpus.append(preprocess_sentence(eng))\n",
    "    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n",
    "\n",
    "print(\"English:\", enc_corpus[100])   # go away !\n",
    "print(\"Spanish:\", dec_corpus[100])   # <start> salga de aqu ! <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32276236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 개수: 24000\n",
      "검증 데이터 개수: 6000\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "# train_test_split을 활용해서 훈련 데이터와 검증 데이터로 분리하기\n",
    "# 1. 토큰화\n",
    "input_tensor, enc_tokenizer = tokenize(enc_corpus)      # 영어 문장\n",
    "target_tensor, dec_tokenizer = tokenize(dec_corpus)    # 스페인어 문장\n",
    "\n",
    "# 2. 훈련/검증 데이터 분할\n",
    "input_train, input_val, target_train, target_val = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. 확인\n",
    "print(f\"훈련 데이터 개수: {len(input_train)}\")\n",
    "print(f\"검증 데이터 개수: {len(input_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a162e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a76b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        # 임베딩 \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "       \n",
    "        # GRU 층\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            enc_units,\n",
    "            return_sequences=True,   # 전체 시퀀스 hidden state 반환 (Attention용)\n",
    "            return_state=True,       # 마지막 hidden state 반환\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, hidden_state=None):     \n",
    "        emb = self.embedding(x)  # [batch, seq_len, embedding_dim]\n",
    "\n",
    "        # 초기 상태가 없다면 0으로 초기화\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.initialize_hidden_state(batch_size=tf.shape(emb)[0])\n",
    "                    \n",
    "        enc_output,state = self.gru(emb, initial_state=hidden_state) # state: [batch, enc_units]\n",
    "       \n",
    "        # 인코더의 모든 time step의 hidden states [batch, seq_len, units]\n",
    "        return enc_output  # ← 단일 텐서만 반환!  \n",
    "        \n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return [tf.zeros((batch_size, self.enc_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981b06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # 임베딩 \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # TODO: Awesome Decoder Modules\n",
    "        self.attention = BahdanauAttention( dec_units)   # Attention 필수 사용!\n",
    "                   \n",
    "        # GRU 층\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            dec_units,\n",
    "            return_sequences=True,   # 전체 시퀀스 hidden state 반환 (Attention용)\n",
    "            return_state=True,       # 마지막 hidden state 반환\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)  # 출력 단어 예측용\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):      \n",
    "        # 1. 디코더 입력 임베딩\n",
    "        x = self.embedding(x)  # [batch, 1, embedding_dim]\n",
    "        \n",
    "        # 2. Attention 계산 (self, h_enc, h_dec):\n",
    "        context_vector, attention_weights = self.attention(enc_out, h_dec)  # context: [batch, enc_units]\n",
    "\n",
    "        # 3. context_vector와 임베딩 결합\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)  # [batch, 1, embedding_dim + enc_units]\n",
    "\n",
    "        # 4. GRU 통과\n",
    "        output, state = self.gru(x)  # output: [batch, 1, dec_units]\n",
    "\n",
    "        # 5. 출력 단어 예측\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))  # [batch, dec_units]\n",
    "        x = self.fc(output)  # [batch, vocab_size]\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e8a5235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 8894)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "255a65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    # from_logits=True: y_pred가 softmax 되기 전 값 (logits : softmax를 거치기 직전값)이기 때문\n",
    "    # → 손실 함수 내부에서 softmax를 자동으로 처리\n",
    "    # reduction='none': 손실을 일괄 평균하지 않고, 토큰마다 개별 손실 계산\n",
    "    # → 나중에 마스크를 씌워서 패딩 위치 손실은 제거할 수 있게 함\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # 패딩(0) 위치는 False\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)             # Boolean mask → float32\n",
    "    loss *= mask                                       # 패딩 위치는 손실 0으로 만듦\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6b350",
   "metadata": {},
   "source": [
    "**훈련 Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455857e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    batch_size = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]   # 마지막 time step의 hidden state를 디코더 초기값으로 사용\n",
    "        \n",
    "        # 디코더 입력 초기값: <start> 토큰\n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            # 손실 누적 (teacher forcing)\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            # 다음 디코더 입력은 정답 (teacher forcing)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    # 역전파 및 최적화\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b97d53",
   "metadata": {},
   "source": [
    "**검증 Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e264bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    batch_size = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "    h_dec = enc_out[:, -1]   # 마지막 time step의 hidden state를 디코더 초기값으로 사용\n",
    "        \n",
    "    # 디코더 입력 초기값: <start> 토큰\n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        # 손실 누적 (teacher forcing)\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        # 다음 디코더 입력은 정답 (teacher forcing)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "  \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72469ba6",
   "metadata": {},
   "source": [
    "**훈련 후 검증**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad796607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # tqdm은 훈련의 진행 과정을 한눈에 볼 수 있게 해주는 라이브러리\n",
    "import random\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, input_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(input_train[idx:idx+BATCH_SIZE],\n",
    "                                target_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm\n",
    "    \n",
    "    \n",
    "    idx_list = list(range(0, input_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(input_val[idx:idx+BATCH_SIZE],\n",
    "                                    target_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d1f66",
   "metadata": {},
   "source": [
    "**Attention Map 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, enc_train. dec_train):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "        \n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder, input_train, target_train)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
